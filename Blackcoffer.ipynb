{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "92d66d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openpyxl\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Step 1: Read input data from \"Input.xlsx\"\n",
    "input_file = \"Input.xlsx\"\n",
    "output_file = \"Output Data Structure.xlsx\"\n",
    "\n",
    "# Load input data from Excel file\n",
    "workbook = openpyxl.load_workbook(input_file)\n",
    "sheet = workbook.active\n",
    "\n",
    "# Get the URLs from the input file\n",
    "urls = []\n",
    "for row in sheet.iter_rows(min_row=2, values_only=True):\n",
    "    url_id, url = row\n",
    "    urls.append((url_id, url))\n",
    "\n",
    "# Step 2: Extract article text from URLs\n",
    "def extract_article_text(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Extract the article text by removing unwanted elements\n",
    "    article_elements = soup.find_all('article')  # Adjust this based on the HTML structure\n",
    "    article_text = ' '.join([element.get_text() for element in article_elements])\n",
    "    return article_text.strip()\n",
    "\n",
    "# Step 3: Save extracted article text in separate files\n",
    "output_folder = \"ExtractedTexts\"\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "for url_id, url in urls:\n",
    "    article_text = extract_article_text(url)\n",
    "    output_filename = os.path.join(output_folder, f\"{url_id}.txt\")\n",
    "\n",
    "    with open(output_filename, 'w', encoding='utf-8') as file:\n",
    "        file.write(article_text)\n",
    "\n",
    "# Step 4: Perform text analysis\n",
    "def calculate_positive_score(text):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    sentiment_scores = sid.polarity_scores(text)\n",
    "    return sentiment_scores['pos']\n",
    "\n",
    "def calculate_negative_score(text):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    sentiment_scores = sid.polarity_scores(text)\n",
    "    return sentiment_scores['neg']\n",
    "\n",
    "def calculate_polarity_score(text):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    sentiment_scores = sid.polarity_scores(text)\n",
    "    return sentiment_scores['compound']\n",
    "\n",
    "def calculate_subjectivity_score(text):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    sentiment_scores = sid.polarity_scores(text)\n",
    "    return sentiment_scores['compound']\n",
    "\n",
    "def calculate_avg_sentence_length(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    if len(sentences) == 0:\n",
    "        return 0\n",
    "    total_words = sum(len(word_tokenize(sentence)) for sentence in sentences)\n",
    "    if total_words == 0:\n",
    "        return 0\n",
    "    return total_words / len(sentences)\n",
    "\n",
    "\n",
    "def calculate_percentage_complex_words(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    if len(tokens) == 0:\n",
    "        return 0\n",
    "    complex_words = [word for word in tokens if len(word) > 2 and word.isalpha()]\n",
    "    return (len(complex_words) / len(tokens)) * 100\n",
    "\n",
    "\n",
    "def calculate_fog_index(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    total_sentences = len(sentences)\n",
    "    if total_sentences == 0:\n",
    "        return 0\n",
    "    total_words = sum(len(word_tokenize(sentence)) for sentence in sentences)\n",
    "    if total_words == 0:\n",
    "        return 0\n",
    "    complex_words = [word for word in word_tokenize(text) if len(word) > 2 and word.isalpha()]\n",
    "    num_complex_words = len(complex_words)\n",
    "    return 0.4 * ((total_words / total_sentences) + 100 * (num_complex_words / total_words))\n",
    "\n",
    "\n",
    "def calculate_avg_words_per_sentence(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    total_sentences = len(sentences)\n",
    "    if total_sentences == 0:\n",
    "        return 0\n",
    "    total_words = sum(len(word_tokenize(sentence)) for sentence in sentences)\n",
    "    return total_words / total_sentences\n",
    "\n",
    "\n",
    "def calculate_complex_word_count(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    complex_words = [word for word in tokens if len(word) > 2 and word.isalpha()]\n",
    "    return len(complex_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3a334985",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Akhil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Akhil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Akhil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c6f58bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_complex_word_count(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    complex_words = [word for word in tokens if len(word) > 2 and word.isalpha()]\n",
    "    return len(complex_words)\n",
    "\n",
    "def calculate_word_count(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return len(tokens)\n",
    "\n",
    "def calculate_syllables_per_word(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    if len(tokens) == 0:\n",
    "        return 0\n",
    "    syllable_count = 0\n",
    "    for token in tokens:\n",
    "        syllable_count += count_syllables(token)\n",
    "    return syllable_count / len(tokens)\n",
    "\n",
    "\n",
    "def calculate_personal_pronouns(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    pronouns = [word for word, tag in pos_tag(tokens) if tag == 'PRP' or tag == 'PRP$']\n",
    "    return len(pronouns)\n",
    "\n",
    "def calculate_avg_word_length(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    if len(tokens) == 0:\n",
    "        return 0\n",
    "    total_length = sum(len(word) for word in tokens)\n",
    "    return total_length / len(tokens)\n",
    "\n",
    "\n",
    "# Helper function to count syllables in a word\n",
    "def count_syllables(word):\n",
    "    vowels = 'aeiouy'\n",
    "    count = 0\n",
    "    word = word.lower().strip(\".:;?!\")\n",
    "\n",
    "    if not word:\n",
    "        return count\n",
    "\n",
    "    if word[0] in vowels:\n",
    "        count += 1\n",
    "\n",
    "    for index in range(1, len(word)):\n",
    "        if word[index] in vowels and word[index - 1] not in vowels:\n",
    "            count += 1\n",
    "\n",
    "    if word.endswith('e'):\n",
    "        count -= 1\n",
    "\n",
    "    if word.endswith('le') and len(word) > 2 and word[-3] not in vowels:\n",
    "        count += 1\n",
    "\n",
    "    if count == 0:\n",
    "        count += 1\n",
    "\n",
    "    return count\n",
    "\n",
    "\n",
    "# Step 5: Compute variables and save in output file\n",
    "output_workbook = openpyxl.Workbook()\n",
    "output_sheet = output_workbook.active\n",
    "\n",
    "# Write headers in the output file\n",
    "output_headers = [\n",
    "    \"URL_ID\",\n",
    "    \"POSITIVE SCORE\",\n",
    "    \"NEGATIVE SCORE\",\n",
    "    \"POLARITY SCORE\",\n",
    "    \"SUBJECTIVITY SCORE\",\n",
    "    \"AVG SENTENCE LENGTH\",\n",
    "    \"PERCENTAGE OF COMPLEX WORDS\",\n",
    "    \"FOG INDEX\",\n",
    "    \"AVG NUMBER OF WORDS PER SENTENCE\",\n",
    "    \"COMPLEX WORD COUNT\",\n",
    "    \"WORD COUNT\",\n",
    "    \"SYLLABLE PER WORD\",\n",
    "    \"PERSONAL PRONOUNS\",\n",
    "    \"AVG WORD LENGTH\"\n",
    "]\n",
    "output_sheet.append(output_headers)\n",
    "\n",
    "import os\n",
    "\n",
    "# Directory paths for stop words files and master dictionary files\n",
    "stop_words_folder = \"StopWords\"\n",
    "master_dict_folder = \"MasterDictionary\"\n",
    "\n",
    "# Load stop words from files in the folder\n",
    "stop_words = set()\n",
    "for filename in os.listdir(stop_words_folder):\n",
    "    file_path = os.path.join(stop_words_folder, filename)\n",
    "    with open(file_path, 'r', encoding='latin-1') as file:\n",
    "        for line in file:\n",
    "            stop_words.add(line.strip())\n",
    "\n",
    "def remove_stop_words(text, stop_words):\n",
    "    \"\"\"\n",
    "    Remove stop words from the given text.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text.\n",
    "        stop_words (set): Set of stop words to remove.\n",
    "\n",
    "    Returns:\n",
    "        str: Text with stop words removed.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    cleaned_words = [word for word in words if word.lower() not in stop_words]\n",
    "    cleaned_text = \" \".join(cleaned_words)\n",
    "    return cleaned_text\n",
    "\n",
    "# Load master dictionary from files in the folder\n",
    "master_dict = set()\n",
    "for filename in os.listdir(master_dict_folder):\n",
    "    file_path = os.path.join(master_dict_folder, filename)\n",
    "    with open(file_path, 'r', encoding='latin-1') as file:\n",
    "        for line in file:\n",
    "            master_dict.add(line.strip())\n",
    "\n",
    "for url_id, url in urls:\n",
    "    # Read the extracted article text from file\n",
    "    input_filename = os.path.join(output_folder, f\"{url_id}.txt\")\n",
    "    with open(input_filename, 'r', encoding='utf-8') as file:\n",
    "        article_text = file.read()\n",
    "\n",
    "    # Clean the text by removing stop words\n",
    "    cleaned_text = remove_stop_words(article_text, stop_words)\n",
    "\n",
    "    # Calculate the variables\n",
    "    positive_score = calculate_positive_score(cleaned_text)\n",
    "    negative_score = calculate_negative_score(cleaned_text)\n",
    "    polarity_score = calculate_polarity_score(cleaned_text)\n",
    "    subjectivity_score = calculate_subjectivity_score(cleaned_text)\n",
    "    avg_sentence_length = calculate_avg_sentence_length(cleaned_text)\n",
    "    percentage_complex_words = calculate_percentage_complex_words(cleaned_text)\n",
    "    fog_index = calculate_fog_index(cleaned_text)\n",
    "    avg_words_per_sentence = calculate_avg_words_per_sentence(cleaned_text)\n",
    "    complex_word_count = calculate_complex_word_count(cleaned_text)\n",
    "    word_count = calculate_word_count(cleaned_text)\n",
    "    syllable_per_word = calculate_syllables_per_word(cleaned_text)\n",
    "    personal_pronouns = calculate_personal_pronouns(cleaned_text)\n",
    "    avg_word_length = calculate_avg_word_length(cleaned_text)\n",
    "\n",
    "    # Prepare the row data for the output file\n",
    "    output_row = [\n",
    "        url_id,\n",
    "        positive_score,\n",
    "        negative_score,\n",
    "        polarity_score,\n",
    "        subjectivity_score,\n",
    "        avg_sentence_length,\n",
    "        percentage_complex_words,\n",
    "        fog_index,\n",
    "        avg_words_per_sentence,\n",
    "        complex_word_count,\n",
    "        word_count,\n",
    "        syllable_per_word,\n",
    "        personal_pronouns,\n",
    "        avg_word_length\n",
    "    ]\n",
    "\n",
    "    # Write the row data to the output file\n",
    "    output_sheet.append(output_row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1c61a377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>28.987013</td>\n",
       "      <td>71.146953</td>\n",
       "      <td>40.053587</td>\n",
       "      <td>28.987013</td>\n",
       "      <td>1588</td>\n",
       "      <td>2232</td>\n",
       "      <td>1.771057</td>\n",
       "      <td>41</td>\n",
       "      <td>5.176523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.9991</td>\n",
       "      <td>0.9991</td>\n",
       "      <td>22.192771</td>\n",
       "      <td>66.829533</td>\n",
       "      <td>35.608922</td>\n",
       "      <td>22.192771</td>\n",
       "      <td>1231</td>\n",
       "      <td>1842</td>\n",
       "      <td>1.518458</td>\n",
       "      <td>74</td>\n",
       "      <td>4.384365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.9939</td>\n",
       "      <td>0.9939</td>\n",
       "      <td>23.263736</td>\n",
       "      <td>69.957487</td>\n",
       "      <td>37.288489</td>\n",
       "      <td>23.263736</td>\n",
       "      <td>1481</td>\n",
       "      <td>2117</td>\n",
       "      <td>1.717997</td>\n",
       "      <td>45</td>\n",
       "      <td>4.930562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.9995</td>\n",
       "      <td>0.9995</td>\n",
       "      <td>20.938144</td>\n",
       "      <td>68.685377</td>\n",
       "      <td>35.849408</td>\n",
       "      <td>20.938144</td>\n",
       "      <td>1395</td>\n",
       "      <td>2031</td>\n",
       "      <td>1.539636</td>\n",
       "      <td>65</td>\n",
       "      <td>4.507632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>26.059524</td>\n",
       "      <td>68.067611</td>\n",
       "      <td>37.650854</td>\n",
       "      <td>26.059524</td>\n",
       "      <td>1490</td>\n",
       "      <td>2189</td>\n",
       "      <td>1.590224</td>\n",
       "      <td>74</td>\n",
       "      <td>4.656464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>146</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.9985</td>\n",
       "      <td>0.9985</td>\n",
       "      <td>24.102041</td>\n",
       "      <td>68.924640</td>\n",
       "      <td>37.210672</td>\n",
       "      <td>24.102041</td>\n",
       "      <td>814</td>\n",
       "      <td>1181</td>\n",
       "      <td>1.687553</td>\n",
       "      <td>34</td>\n",
       "      <td>5.041490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>147</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.9995</td>\n",
       "      <td>0.9995</td>\n",
       "      <td>30.515625</td>\n",
       "      <td>65.642601</td>\n",
       "      <td>38.463290</td>\n",
       "      <td>30.515625</td>\n",
       "      <td>1282</td>\n",
       "      <td>1953</td>\n",
       "      <td>1.590886</td>\n",
       "      <td>40</td>\n",
       "      <td>4.682028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>148</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.9533</td>\n",
       "      <td>0.9533</td>\n",
       "      <td>21.602941</td>\n",
       "      <td>67.665078</td>\n",
       "      <td>35.707208</td>\n",
       "      <td>21.602941</td>\n",
       "      <td>994</td>\n",
       "      <td>1469</td>\n",
       "      <td>1.640572</td>\n",
       "      <td>22</td>\n",
       "      <td>4.724302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>149</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.9990</td>\n",
       "      <td>0.9990</td>\n",
       "      <td>32.300000</td>\n",
       "      <td>71.310630</td>\n",
       "      <td>41.444252</td>\n",
       "      <td>32.300000</td>\n",
       "      <td>691</td>\n",
       "      <td>969</td>\n",
       "      <td>1.786378</td>\n",
       "      <td>26</td>\n",
       "      <td>5.250774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>150</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.9993</td>\n",
       "      <td>0.9993</td>\n",
       "      <td>18.802817</td>\n",
       "      <td>69.662921</td>\n",
       "      <td>35.386295</td>\n",
       "      <td>18.802817</td>\n",
       "      <td>930</td>\n",
       "      <td>1335</td>\n",
       "      <td>1.628464</td>\n",
       "      <td>35</td>\n",
       "      <td>4.721348</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     URL_ID  POSITIVE SCORE  NEGATIVE SCORE  POLARITY SCORE  \\\n",
       "0        37           0.146           0.040          0.9997   \n",
       "1        38           0.153           0.076          0.9991   \n",
       "2        39           0.077           0.035          0.9939   \n",
       "3        40           0.140           0.042          0.9995   \n",
       "4        41           0.145           0.047          0.9997   \n",
       "..      ...             ...             ...             ...   \n",
       "109     146           0.133           0.044          0.9985   \n",
       "110     147           0.111           0.018          0.9995   \n",
       "111     148           0.100           0.080          0.9533   \n",
       "112     149           0.159           0.011          0.9990   \n",
       "113     150           0.184           0.071          0.9993   \n",
       "\n",
       "     SUBJECTIVITY SCORE  AVG SENTENCE LENGTH  PERCENTAGE OF COMPLEX WORDS  \\\n",
       "0                0.9997            28.987013                    71.146953   \n",
       "1                0.9991            22.192771                    66.829533   \n",
       "2                0.9939            23.263736                    69.957487   \n",
       "3                0.9995            20.938144                    68.685377   \n",
       "4                0.9997            26.059524                    68.067611   \n",
       "..                  ...                  ...                          ...   \n",
       "109              0.9985            24.102041                    68.924640   \n",
       "110              0.9995            30.515625                    65.642601   \n",
       "111              0.9533            21.602941                    67.665078   \n",
       "112              0.9990            32.300000                    71.310630   \n",
       "113              0.9993            18.802817                    69.662921   \n",
       "\n",
       "     FOG INDEX  AVG NUMBER OF WORDS PER SENTENCE  COMPLEX WORD COUNT  \\\n",
       "0    40.053587                         28.987013                1588   \n",
       "1    35.608922                         22.192771                1231   \n",
       "2    37.288489                         23.263736                1481   \n",
       "3    35.849408                         20.938144                1395   \n",
       "4    37.650854                         26.059524                1490   \n",
       "..         ...                               ...                 ...   \n",
       "109  37.210672                         24.102041                 814   \n",
       "110  38.463290                         30.515625                1282   \n",
       "111  35.707208                         21.602941                 994   \n",
       "112  41.444252                         32.300000                 691   \n",
       "113  35.386295                         18.802817                 930   \n",
       "\n",
       "     WORD COUNT  SYLLABLE PER WORD  PERSONAL PRONOUNS  AVG WORD LENGTH  \n",
       "0          2232           1.771057                 41         5.176523  \n",
       "1          1842           1.518458                 74         4.384365  \n",
       "2          2117           1.717997                 45         4.930562  \n",
       "3          2031           1.539636                 65         4.507632  \n",
       "4          2189           1.590224                 74         4.656464  \n",
       "..          ...                ...                ...              ...  \n",
       "109        1181           1.687553                 34         5.041490  \n",
       "110        1953           1.590886                 40         4.682028  \n",
       "111        1469           1.640572                 22         4.724302  \n",
       "112         969           1.786378                 26         5.250774  \n",
       "113        1335           1.628464                 35         4.721348  \n",
       "\n",
       "[114 rows x 14 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel(\"Output Data Structure.xlsx\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c8a993",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
